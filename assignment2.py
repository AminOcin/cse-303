# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q673vsiwLfcL1Z1lg4UhzHTNLWUS6WQl
"""

#importing all the required libraries and packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from pandas.plotting import scatter_matrix
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector, make_column_transformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import gdown

df = pd.read_csv('/content/AI_in_HealthCare_Dataset.csv')

df.shape

df.info()

df.isnull().sum()

df.head(

)

df.columns

#mode_value = df['13. Physical activity '].mode()

# Print the mode value
print("Mode of the column 'Allergies':", df['Allergies'].mode())
df= df.fillna(value=str(df['Allergies'].mode()))

df.isnull().sum()

for col in df.columns[0:]:
  print (col + ":" + str(len(df[col].unique())) + ' labels')

def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

for column in df.select_dtypes(include=['float64', 'int64']).columns:
    df = remove_outliers_iqr(df, column)

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

scaler = MinMaxScaler()
continuous_columns = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
df[continuous_columns] = scaler.fit_transform(df[continuous_columns])

# Example: Creating a new feature based on Age
df['Age_Group'] = pd.cut(df['Age'], bins=[0, 18, 35, 50, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])
df['Age_Group'] = LabelEncoder().fit_transform(df['Age_Group'])

import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix
correlation_matrix = df.corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Get the absolute value of the correlations with the target variable
correlation_with_target = correlation_matrix['Patient_Satisfaction'].abs().sort_values(ascending=False)
print("Correlation with Target:\n", correlation_with_target)

X = df.drop(columns=['Patient_Satisfaction', 'Patient_ID'])
y = df['Patient_Satisfaction']

# Train a Linear Regression model
model = LinearRegression()
model.fit(X, y)

# Get feature importances (coefficients in case of Linear Regression)
feature_importances = pd.Series(model.coef_, index=X.columns)
important_features = feature_importances.nlargest(10)

# Plotting the top 10 important features
plt.figure(figsize=(10, 6))
sns.barplot(x=important_features, y=important_features.index)
plt.title('Top 10 Important Features')
plt.show()

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in df.columns:
    if df[column].dtype == 'object':
        le = LabelEncoder()
        df[column] = le.fit_transform(df[column])
        label_encoders[column] = le
df['Patient_Satisfaction'] = le.fit_transform(df['Patient_Satisfaction'])
df.head()

from sklearn.model_selection import train_test_split

X = df.drop(columns=['Patient_Satisfaction'], axis=1)
y = df['Patient_Satisfaction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report


model1 = RandomForestClassifier(random_state=42)
model1.fit(X_train, y_train)
y_pred = model1.predict(X_test)
print(f'Random Forest- Accuracy: {accuracy_score(y_test, y_pred)}')
print(classification_report(y_test, y_pred))

models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    #'Random Forest': RandomForestClassifier(random_state=42),
    'Support Vector Machine': SVC(random_state=42)
}
for name, model in models.items():
    model = model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f'{name} - Accuracy: {accuracy_score(y_test, y_pred)}')
    print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print(f'Best Model: {best_model}')

y_pred = best_model.predict(X_test)
print('Best Model - Accuracy:', accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

continuous_columns = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
df[continuous_columns] = scaler.fit_transform(df[continuous_columns])


# Example: Creating a new feature based on Age
df['Age_Group'] = pd.cut(df['Age'], bins=[0, 18, 35, 50, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])
df['Age_Group'] = LabelEncoder().fit_transform(df['Age_Group'])


from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

X = df.drop(columns=['Patient_Satisfaction', 'Patient_ID'])
y = df['Patient_Satisfaction']

model = RandomForestClassifier(random_state=42)
model.fit(X, y)

feature_importances = pd.Series(model.feature_importances_, index=X.columns)
important_features = feature_importances.nlargest(10)

plt.figure(figsize=(10, 6))
sns.barplot(x=important_features, y=important_features.index)
plt.title('Top 10 Important Features')
plt.show()


from sklearn.model_selection import train_test_split

X = df.drop(columns=['Patient_Satisfaction_Level', 'Patient_ID'])
y = df['Patient_Satisfaction_Level']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

models = {
    'Logistic Regression': Log VisticRegression(random_state=42),
    'Random Forest': RandomFores5tClassifier(random_state=42),
    'Support Vector Machine': SVC(random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f'{name} - Accuracy: {accuracy_score(y_test, y_pred)}')
    print(classification_report(y_test, y_pred))


from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print(f'Best Model: {best_model}')

y_pred = best_model.predict(X_test)
print('Best Model - Accuracy:', accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


import shap

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# Bar plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Dot plot
shap.summary_plot(shap_values, X_test)



X_reg = df.drop(columns=['AI_Diagnosis_Confidence', 'Patient_ID'])
y_reg = df['AI_Diagnosis_Confidence']

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

reg = RandomForestRegressor(random_state=42)
reg.fit(X_train_reg, y_train_reg)

y_pred_reg = reg.predict(X_test_reg)
print("MSE:", mean_squared_error(y_test_reg, y_pred_reg))


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

reg = RandomForestRegressor(random_state=42)
reg.fit(X_train_reg, y_train_reg)

y_pred_reg = reg.predict(X_test_reg)
print("MSE:", mean_squared_error(y_test_reg, y_pred_reg))


explainer_reg = shap.TreeExplainer(reg)
shap_values_reg = explainer_reg.shap_values(X_test_reg)

# Bar plot
shap.summary_plot(shap_values_reg, X_test_reg, plot_type="bar")

# Dot plot
shap.summary_plot(shap_values_reg, X_test_reg)

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print(f'Best Model: {best_model}')

y_pred = best_model.predict(X_test)
print('Best Model - Accuracy:', accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

!pip install shap

import shap
explainer = shap.TreeExplainer(model1)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test,plot_type="dot")

df.head()

x = df.drop(['Patient_Satisfaction', 'AI_Diagnosis_Confidence'], axis=1)
y = df['AI_Diagnosis_Confidence']

"""# problem 2"""

from sklearn.model_selection import train_test_split

x1 = df.drop(columns=['AI_Diagnosis_Confidence'], axis=1)
y1 = df['AI_Diagnosis_Confidence']

x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=42)


print(x1_train.shape, x1_test.shape, y1_train.shape, y1_test.shape)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

reg = RandomForestRegressor(random_state=42)
reg.fit(x1_train, y1_train)

y_pred_reg = reg.predict(x1_test)
print("MSE:", mean_squared_error(y1_test, y_pred_reg))